---
jupyter:
  jupytext:
    formats: ipynb,md
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.16.6
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import rewardgym
from rewardgym import agents, get_env
from rewardgym.psychopy_core import run_task
import seaborn as sns
import warnings
from copy import deepcopy

from typing import Union, Tuple, List
from rewardgym.utils import check_seed, get_stripped_graph

from rewardgym.environments.visualizations import plot_env_graph

from tqdm.auto import tqdm

seed = np.random.default_rng(2025)
```

```python
class ValenceQAgent:
    """
    A reinforcement learning agent implementing a Valence-based Q-learning algorithm.
    The agent maintains state-action values (Q-values) and updates them using different
    learning rates for positive and negative temporal differences.
    """

    def __init__(
        self,
        learning_rate_pos: float,
        learning_rate_neg: float,
        temperature: float,
        discount_factor: float = 0.99,
        action_space: int = 2,
        state_space: int = 2,
        seed: Union[int, np.random.Generator] = 1000,
    ):
        """
        Initializes the ValenceQAgent with parameters for learning, exploration, and environment size.

        Parameters
        ----------
        learning_rate_pos : float
            The learning rate used for updating Q-values when the temporal difference is positive.
        learning_rate_neg : float
            The learning rate used for updating Q-values when the temporal difference is negative.
        temperature : float
            The softmax temperature controlling exploration during action selection.
        discount_factor : float, optional
            The discount factor for future rewards, by default 0.99.
        action_space : int, optional
            The number of actions available in the environment, by default 2.
        state_space : int, optional
            The number of states in the environment, by default 2.
        seed : Union[int, np.random.Generator], optional
            Seed or random number generator for reproducibility, by default 1000.
        """

        self.q_values = np.zeros((state_space, action_space)) + 1 / action_space

        self.n_states = state_space
        self.n_actions = action_space
        self.lr_neg = learning_rate_neg
        self.lr_pos = learning_rate_pos

        self.temperature = temperature
        self.discount_factor = discount_factor

        self.rng = check_seed(seed)

        self.training_error = []

    def get_action(self, obs: Tuple[int, int, bool], avail_actions: List = None) -> int:
        """
        Selects an action based on the current state observation using a softmax policy.

        Parameters
        ----------
        obs : Tuple[int, int, bool]
            The current state observation represented as a tuple.
        avail_actions : list, optional
            A list of available actions. If None, all actions are assumed available, by default None.

        Returns
        -------
        int
            The selected action index.
        """

        prob = self.get_probs(obs, avail_actions)

        a = self.rng.choice(np.arange(len(prob)), p=prob)

        return a

    def get_probs(self, obs: Tuple[int, int, bool], avail_actions: List = None):
        """
        Computes the softmax probability distribution over actions based on Q-values.

        Parameters
        ----------
        obs : Tuple[int, int, bool]
            The current state observation represented as a tuple.
        avail_actions : list, optional
            A list of available actions. If None, all actions are assumed available, by default None.

        Returns
        -------
        np.ndarray
            A probability distribution over the available actions.
        """

        prob = np.zeros_like(self.q_values[obs])

        if avail_actions is None:
            avail_actions = np.arange(len(self.q_values[obs]))

        qval = self.q_values[obs][avail_actions]
        qval = qval - np.mean(qval)
        qs = np.exp(qval * self.temperature)

        if any(~np.isfinite(qs)):
            warnings.warn("Overflow in softmax, replacing with max / min value.")
            qs[np.isposinf(qs)] = np.finfo(float).max
            qs[np.isneginf(qs)] = np.finfo(float).min

        prob[avail_actions] = qs / np.sum(qs)

        return prob

    def update(
        self,
        obs: Tuple[int, int, bool],
        action: int,
        reward: float,
        terminated: bool,
        next_obs: Tuple[int, int, bool],
    ):
        """
        Updates the Q-value for a given action taken in a particular state using a
        valence-based learning rate (positive or negative) depending on the temporal difference.

        Parameters
        ----------
        obs : Tuple[int, int, bool]
            The current state observation represented as a tuple.
        action : int
            The action taken in the current state.
        reward : float
            The reward received after taking the action.
        terminated : bool
            Whether the episode has ended after this action.
        next_obs : Tuple[int, int, bool]
            The next state observation after taking the action.

        Returns
        -------
        np.ndarray
            The updated Q-value table.
        """

        future_q_value = (not terminated) * np.max(self.q_values[next_obs])

        temporal_difference = (
            reward + self.discount_factor * future_q_value - self.q_values[obs][action]
        )
        if temporal_difference > 0:
            q_update = self.lr_pos * temporal_difference
        elif temporal_difference <= 0:
            q_update = self.lr_neg * temporal_difference

        self.q_values[obs][action] = self.q_values[obs][action] + q_update
        self.training_error.append(temporal_difference)

        return self.q_values


class ValenceQAgent_eligibility(ValenceQAgent):
    def __init__(
        self,
        learning_rate_pos: float,
        learning_rate_neg: float,
        temperature: float,
        discount_factor: float = 0.99,
        eligibility_decay: float = 0.0,
        reset_traces: bool = True,
        action_space: int = 2,
        state_space: int = 2,
        seed: Union[int, np.random.Generator] = 1000,
    ):
        """
        Initializes the ValenceQAgent with eligibility traces, and
        parameters for learning, exploration, and environment size.

        Parameters
        ----------
        learning_rate_pos : float
            The learning rate used for updating Q-values when the temporal difference is positive.
        learning_rate_neg : float
            The learning rate used for updating Q-values when the temporal difference is negative.
        temperature : float
            The softmax temperature controlling exploration during action selection.
        discount_factor : float, optional
            The discount factor for future rewards, by default 0.99.
        eligibility_decay : float, optional
            The decay rate of the eligibility traces, by default 0.0.
        reset_traces : bool, optional
            Whether to reset the eligibility traces each episode or not, by default True.
        action_space : int, optional
            The number of actions available in the environment, by default 2.
        state_space : int, optional
            The number of states in the environment, by default 2.
        seed : Union[int, np.random.Generator], optional
            Seed or random number generator for reproducibility, by default 1000.
        """

        super().__init__(
            learning_rate_pos=learning_rate_pos,
            learning_rate_neg=learning_rate_neg,
            temperature=temperature,
            discount_factor=discount_factor,
            action_space=action_space,
            state_space=state_space,
            seed=seed,
        )

        self.eligibility_traces = np.zeros((state_space, action_space))
        self.eligibility_decay = eligibility_decay
        self.reset_traces = reset_traces

    def update(
        self,
        obs: Tuple[int, int, bool],
        action: int,
        reward: float,
        terminated: bool,
        next_obs: Tuple[int, int, bool],
    ):
        """Updates the Q-value of an action."""
        future_q_value = (not terminated) * np.max(self.q_values[next_obs])
        temporal_difference = (
            reward + self.discount_factor * future_q_value - self.q_values[obs][action]
        )

        if temporal_difference > 0:
            q_update = self.lr_pos * temporal_difference
        elif temporal_difference <= 0:
            q_update = self.lr_neg * temporal_difference
        else:
            q_update = np.nan

        self.eligibility_traces[obs][action] = self.eligibility_traces[obs][action] + 1

        self.q_values = self.q_values + q_update * self.eligibility_traces

        self.eligibility_traces = (
            self.eligibility_traces * self.discount_factor * self.eligibility_decay
        )

        if terminated and self.reset_traces:
            self.eligibility_traces = np.zeros_like(self.eligibility_traces)

        self.training_error.append(temporal_difference)

        return self.q_values

  
class ValenceHybridAgent(ValenceQAgent):
    """
    A hybrid reinforcement learning agent combining model-based (MB) and model-free (MF) learning.
    The agent maintains a state-action transition table for model-based learning and a Q-value
    table for model-free learning. The combination is controlled by a hybrid parameter that blends
    the two approaches.
    """

    def __init__(
        self,
        learning_rate_mb: float,
        learning_rate_mf_pos: float,
        learning_rate_mf_neg: float,
        temperature: float,
        discount_factor: float = 0.99,
        eligibility_decay: float = 0.0,
        reset_traces: bool = True,
        hybrid: float = 1.0,
        action_space: int = 2,
        state_space: int = 2,
        seed: Union[int, np.random.Generator] = 1000,
        graph=None,
        use_fixed=False,
    ):
        """
        Initializes the HybridAgent with parameters for both model-based and model-free learning.
        The agent maintains both state-action Q-values and state-action-state transition probabilities.

        Parameters
        ----------
        learning_rate_mb : float
            The learning rate for updating the state-action-state transition model (model-based learning).
        learning_rate_mf : Union[float, List]
            The learning rate(s) for model-free learning. Can be a single float or a list with two values
            for positive and negative learning rates (if using valence-based Q-learning).
        temperature : float
            The softmax temperature used to control exploration during action selection.
        discount_factor : float, optional
            The discount factor for future rewards, by default 0.99.
        eligibility_decay : float, optional
            The decay rate for eligibility traces in model-free learning, by default 0.0.
        reset_traces : bool, optional
            Whether to reset the eligibility traces after an episode, by default True.
        hybrid : float, optional
            A parameter that controls the balance between model-based (MB) and model-free (MF) learning,
            by default 1.0 (fully MB).
        action_space : int, optional
            The number of actions available in the environment, by default 2.
        state_space : int, optional
            The number of states in the environment, by default 2.
        seed : Union[int, np.random.Generator], optional
            Seed or random number generator for reproducibility, by default 1000.
        graph : dict, optional
            A dictionary representing the environment's state-action transition graph. If None, transitions
            are initialized randomly, by default None.
        use_fixed : bool, optional
            Whether to use fixed transition probabilities (based on a given graph), by default False.
        """
        self.t_values = np.zeros((state_space, action_space, state_space))

        if graph is not None:
            for k in graph.keys():
                actions = list(graph[k].keys())

                for a in actions:
                    loc = graph[k][a]

                    if isinstance(graph[k][a], tuple):
                        prob = graph[k][a][1]
                        loc = graph[k][a][0]
                    else:
                        prob = None

                    loc = [loc] if isinstance(loc, int) else loc
                    ln = len(loc)

                    if use_fixed and prob is not None:
                        for n, j in enumerate(loc):
                            if n == 0:
                                self.t_values[k, a, j] = prob
                            else:
                                self.t_values[k, a, j] = (1 - prob) / max([1, ln - 1])
                    else:
                        for j in loc:
                            self.t_values[k, a, j] = 1 / max([1, ln])

        self.discount_factor = discount_factor

        self.q_agent = ValenceQAgent_eligibility(
            learning_rate_neg=learning_rate_mf_neg,
            learning_rate_pos=learning_rate_mf_pos,
            temperature=temperature,
            discount_factor=self.discount_factor,
            action_space=action_space,
            state_space=state_space,
            eligibility_decay=eligibility_decay,
            reset_traces=reset_traces,
        )

        self.q_values = np.zeros((state_space, action_space))
        self.lr = learning_rate_mb
        self.action_space = action_space
        self.temperature = temperature
        self.hybrid = hybrid
        self.rng = check_seed(seed)

        self.training_error = []

    def get_probs(self, obs, avail_actions=None):
        """
        Computes the softmax probability distribution over actions based on a combination of
        model-based (MB) and model-free (MF) Q-values, controlled by the hybrid parameter.

        Parameters
        ----------
        obs : Tuple[int, int, bool]
            The current state observation.
        avail_actions : list, optional
            A list of available actions. If None, all actions are considered available, by default None.

        Returns
        -------
        np.ndarray
            A probability distribution over the available actions.
        """
        prob = np.zeros_like(self.q_values[obs])

        if avail_actions is None:
            avail_actions = np.arange(len(self.q_values[obs]))

        qval_mb = self.q_values[obs][avail_actions]
        qval_mf = self.q_agent.q_values[obs][avail_actions]

        qval = qval_mb * self.hybrid + qval_mf * (1 - self.hybrid)

        qval = qval - np.mean(qval)

        qs = np.exp(qval * self.temperature)

        if any(~np.isfinite(qs)):
            warnings.warn("Overflow in softmax, replacing with max / min value.")
            qs[np.isposinf(qs)] = np.finfo(float).max
            qs[np.isneginf(qs)] = np.finfo(float).min

        prob[avail_actions] = qs / np.sum(qs)

        return prob

    def update(
        self,
        obs: Tuple[int, int, bool],
        action: int,
        reward: float,
        terminated: bool,
        next_obs: Tuple[int, int, bool],
    ):
        """
        Updates the Q-values for both model-based and model-free learning and updates
        the state-action-state transition probabilities for model-based learning.

        Parameters
        ----------
        obs : Tuple[int, int, bool]
            The current state observation.
        action : int
            The action taken in the current state.
        reward : float
            The reward received after taking the action.
        terminated : bool
            Whether the episode has terminated.
        next_obs : Tuple[int, int, bool]
            The next state observation after taking the action.

        Returns
        -------
        np.ndarray
            The updated Q-value table.
        """

        self.q_agent.update(obs, action, reward, terminated, next_obs)

        state_prediction_error = 1 - self.t_values[obs][action][next_obs]

        for n in range(self.t_values.shape[-1]):
            if n == next_obs:
                self.t_values[obs][action][n] = (
                    self.t_values[obs][action][n] + self.lr * state_prediction_error
                )
            else:
                self.t_values[obs][action][n] = self.t_values[obs][action][n] * (
                    1 - self.lr
                )

        if not terminated:
            qval_mb = 0
            for s2 in range(self.t_values.shape[-1]):
                qval_mb += self.t_values[obs, action, s2] * (
                    reward + np.max(self.q_agent.q_values[s2])
                )
            self.q_values[obs][action] = qval_mb
        else:
            self.q_values[obs][action] = self.q_agent.q_values[obs][action]

        self.training_error.append(state_prediction_error)

        return self.q_values


class HybridAgent(ValenceHybridAgent):
    """
    A hybrid reinforcement learning agent combining model-based (MB) and model-free (MF) learning.
    The agent maintains a state-action transition table for model-based learning and a Q-value
    table for model-free learning. The combination is controlled by a hybrid parameter that blends
    the two approaches.
    """

    def __init__(
        self,
        learning_rate_mb: float,
        learning_rate_mf: float,
        temperature: float,
        discount_factor: float = 0.99,
        eligibility_decay: float = 0.0,
        reset_traces: bool = True,
        hybrid: float = 1.0,
        action_space: int = 2,
        state_space: int = 2,
        seed: Union[int, np.random.Generator] = 1000,
        graph=None,
        use_fixed=False,
    ):
        """
        Initializes the HybridAgent with parameters for both model-based and model-free learning.
        The agent maintains both state-action Q-values and state-action-state transition probabilities.

        Parameters
        ----------
        learning_rate_mb : float
            The learning rate for updating the state-action-state transition model (model-based learning).
        learning_rate_mf : Union[float, List]
            The learning rate(s) for model-free learning. Can be a single float or a list with two values
            for positive and negative learning rates (if using valence-based Q-learning).
        temperature : float
            The softmax temperature used to control exploration during action selection.
        discount_factor : float, optional
            The discount factor for future rewards, by default 0.99.
        eligibility_decay : float, optional
            The decay rate for eligibility traces in model-free learning, by default 0.0.
        reset_traces : bool, optional
            Whether to reset the eligibility traces after an episode, by default True.
        hybrid : float, optional
            A parameter that controls the balance between model-based (MB) and model-free (MF) learning,
            by default 1.0 (fully MB).
        action_space : int, optional
            The number of actions available in the environment, by default 2.
        state_space : int, optional
            The number of states in the environment, by default 2.
        seed : Union[int, np.random.Generator], optional
            Seed or random number generator for reproducibility, by default 1000.
        graph : dict, optional
            A dictionary representing the environment's state-action transition graph. If None, transitions
            are initialized randomly, by default None.
        use_fixed : bool, optional
            Whether to use fixed transition probabilities (based on a given graph), by default False.
        """
        super().__init__(
            learning_rate_mf_pos=learning_rate_mf,
            learning_rate_mf_neg=learning_rate_mf,
            learning_rate_mb=learning_rate_mb,
            eligibility_decay=eligibility_decay,
            reset_traces=reset_traces,
            hybrid=hybrid,
            graph=graph,
            use_fixed=use_fixed,
            temperature=temperature,
            discount_factor=discount_factor,
            action_space=action_space,
            state_space=state_space,
            seed=seed,
        )


class RandomAgent(ValenceQAgent):
    def __init__(
        self,
        bias: float = None,
        action_space: int = 2,
        state_space: int = 2,
        seed: Union[int, np.random.Generator] = 1000,
    ) -> None:
        self.bias = bias

        self.action_space = action_space
        self.state_space = state_space
        self.rng = check_seed(seed)

    def update(self, *args, **kwargs):
        return None

    def get_probs(self, obs, avail_actions=None):
        if avail_actions is None:
            avail_actions = np.arange(self.action_space)

        action_probs = np.zeros(self.action_space)

        if len(avail_actions) > 1:
            action_probs[avail_actions[0]] = self.bias
            action_probs[avail_actions[1:]] = (1 - self.bias) / (len(avail_actions) - 1)
        else:
            action_probs[avail_actions] = 1

        prob = action_probs

        return prob
```

# Setting up your own agent

```python
from rewardgym.environments import BaseEnv
from rewardgym.reward_classes import BaseReward
# We use dictionaries as graph representation.
# Nodes are represented by the keys, and edges are represented by lists.
# For the bandit task, there is an initial decision point (machine 1 or 2).
# Resulting in an initial and two terminal nodes. Edges define the possible actions.
env_graph = {0: [1, 2], 1: [], 2: []}
# The terminal nodes are used to realize rewards.
reward1 = BaseReward(reward=[0, 1], p=[0.2, 0.8], seed=2222)
reward2 = BaseReward(reward=[0, 1], p=[0.5, 0.5], seed=3333)
# This creates the reward dictionary necessary for the environment.

reward_locs = {1: reward1, 2: reward2}
env = BaseEnv(environment_graph=env_graph, reward_locations=reward_locs)
n_episodes = 100

actions = []
rewards = []

for t in range(n_episodes):
    obs, info = env.reset(agent_location=0)
    done = False
    while not done:
        # Sample from the action space, which is automatically created from the edges.
        action = env.action_space.sample()
        next_obs, reward, terminated, truncated, info = env.step(action)
        done = terminated or truncated
        actions.append(action)
        rewards.append(reward)

plt.subplot(1, 2, 1)
plt.plot(rewards)
plt.scatter(np.arange(n_episodes), actions)
plt.subplot(1, 2, 2)
plot_env_graph(env)
```

# Plotting the environment

```python
from rewardgym.environments.visualizations import plot_env_graph
plot_env_graph(get_env('posner'))
```

# Helper functions to run simulation data

```python
from rewardgym import get_configs

def run_episode(env, agent, config, n):

    episode = []
    
    obs, info = env.reset(agent_location=0, condition=config)

    done = False

    while not done:

        old_info = info    
        action = agent.get_action(obs, info["avail-actions"])

        next_obs, reward, terminated, truncated, info = env.step(
            action, step_reward=False
        )

        episode.append((action, reward, obs, next_obs, terminated, old_info["avail-actions"]))

        agent.update(obs, action, reward, terminated, next_obs)
        
        done = terminated or truncated
        obs = next_obs

    return episode


def run_episodes(env, agent):

    settings = get_configs(env.name)(5)
    actions = []
    rewards = [] 
    obs0 = []
    obs1 = []
    terminated = []
    avail_actions = []
    n_episodes = settings["ntrials"]

    for n in range(n_episodes):

        episode_data = run_episode(env, agent, config=settings["condition_dict"][settings["condition"][n]], n=n)

        for ep in episode_data:
            for stp, ll in zip(ep, [actions, rewards, obs0, obs1, terminated, avail_actions]):
                ll.append(stp)

    return actions, rewards, obs0, obs1, terminated, avail_actions

```

# Simulation

```python
learning_rates_pos = [0.1, 0.15, 0.2] * 3 
learning_rates_neg = [0.2, 0.15, 0.1] * 3
hybrid = [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5]
bias = [0.5]

n_agents = 1


simulation_data_core = ['task', 'agent', 'reward', 'params', "agent_model", "agent_valence"]
simulation_data_behav = ['actions', 'rewards', 'obs0', 'obs1', 'terminated', 'avail-actions']
tasks = ['risk-sensitive', 'gonogo', 'two-step']

agent_data = {i : [] for i in simulation_data_core + simulation_data_behav}

cc = 0

for task in tasks:

    env = get_env(task)
    
    agents = [ValenceHybridAgent(
                learning_rate_mf_neg=lrn, 
                learning_rate_mf_pos=lrp, 
                learning_rate_mb=0, 
                temperature=4, 
                eligibility_decay=0.5, 
                discount_factor=0.9, 
                graph=env.full_graph,
                state_space=env.n_states, 
                hybrid=hb, 
                use_fixed=True,
                action_space=env.n_actions) 
              for lrp, lrn, hb in zip(learning_rates_pos, learning_rates_neg, hybrid)]

    agent_names = [i + '_' + j  for j in ['mf', 'hyp', 'mb'] for i in ['pessimistic', 'neutral', 'optimistic'] ]

    ragents = [RandomAgent(bias=bb,  action_space=env.n_actions,
                           state_space=env.n_states) for bb in bias]

    ragents_names = ['random'] * len(bias)

    params = [bias] + [(lrn, lrp, hb) for lrp, lrn, hb in zip(learning_rates_pos, learning_rates_neg, hybrid)]
    
    agent_names = ragents_names + agent_names 
    agents =  ragents + agents

    for ag, agn, par in zip(agents, agent_names, params):
        for _ in range(2):
            acti, rew, obs0, obs1, term, avail = run_episodes(env, ag)
            agent_data['task'].append(task)
            agent_data['agent'].append(agn)
            agent_data['agent_model'].append(agn.split('_')[1] if agn != "random" else "random")
            agent_data['agent_valence'].append(agn.split('_')[0] if agn != "random" else "random")
            agent_data['reward'].append(np.sum(rew))
            agent_data['params'].append(par)

            for dat, mes in zip([acti, rew, obs0, obs1, term, avail], simulation_data_behav):
                agent_data[mes].append(dat)
```

# Agent performance on task

```python
agent_df = pd.DataFrame(agent_data)
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
axes = axes.flatten()

sns.barplot(agent_df.query("task == 'risk-sensitive'"), x='agent_valence', y="reward", ax=axes[0])
sns.barplot(agent_df.query("task == 'gonogo'"), x='agent_valence', y="reward", ax=axes[1])
sns.barplot(agent_df.query("task == 'two-step'"), x='agent_model', y="reward", ax=axes[2])
```

# Parameter and model recovery

## Optimization is part of rewardCoach

```python
import scipy

def loglikelihood_binary(x, *args):
    # Extract the arguments as they are passed by scipy.optimize.minimize
    (
        agent,
        parameter_names,
        agent_settings,
        actions,
        rewards,
        starting,
        obs,
        terminated,
        avail_actions,
    ) = args

    agent_settings.update({i: j for i, j in zip(parameter_names, x)})

    agent = agent(**agent_settings)

    # Initialize values
    logp_actions = np.zeros(len(actions))

    for t, (a, r, o, ot1, term, ava) in enumerate(
        zip(actions, rewards, starting, obs, terminated, avail_actions)
    ):
        # Apply the softmax transformation
        logp_action = np.log(agent.get_probs(o, ava) + np.finfo(float).eps)

        logp_actions[t] = logp_action[a]
        agent.update(o, a, r, term, ot1)

    # Return the negative log likelihood of all observed actions
    return -np.sum(logp_actions[:])


def optimize_loglikelihood(
    actions,
    rewards,
    obs0,
    obs1,
    terminated,
    avail_actions,
    agent_settings,
    parameter_names,
    parameter_settings,
    agent,
    method="L-BFGS-B",
):
    initial_params = [parameter_settings[i]["initial"] for i in parameter_names]
    bounds = tuple(parameter_settings[i]["bounds"] for i in parameter_names)

    result = scipy.optimize.minimize(
        loglikelihood_binary,
        initial_params,
        args=(
            agent,
            parameter_names,
            agent_settings,
            actions,
            rewards,
            obs0,
            obs1,
            terminated,
            avail_actions,
        ),
        method=method,
        bounds=bounds,
    )

    return result
```

## Optimization loop

```python
params_to_track = ['learning_rate_mf_pos', 'learning_rate_mf_neg', 'learning_rate_mf', 'temperature']

recovery_data = {"task" :[], 
                 "set": [], 
                 "rcov_agent": [], 
                 "learning_rate_mf_pos" : [], 
                 "learning_rate_mf_neg": [],
                 "learning_rate_mf": [], 
                 "temperature": [],
                 "lln": [],
                 "bic": [],
                 "orig_params": [], 
                 "orig_agent": []}

# Setting up parameter dictionaries:

hybrid_params = {"mf": 0.0, "mb": 1.0, "hyb": 0.5}
agent_class = {"valence": ValenceHybridAgent, "q": HybridAgent}

agent_fixed_params = {"hybrid":{
                      "learning_rate_mb":0,
                      "eligibility_decay": 0.5, 
                      "discount_factor": 0.9, 
                      "use_fixed": True,
                      "hybrid": 1.0}}

agent_free_param_names = {}
agent_free_param_names["valence"] = ["learning_rate_mf_pos", "learning_rate_mf_neg",  "temperature"]
agent_free_param_names["q"] = ["learning_rate_mf",  "temperature"]

agent_free_params = {}
agent_free_params["valence"] = {"learning_rate_mf_pos": {"initial": 0.5, "bounds": [0, 1]},
                     "learning_rate_mf_neg": {"initial": 0.5, "bounds": [0, 1]},
                     "temperature": {"initial": 3, "bounds": [0.1, 9.0]}}
agent_free_params["q"] = {"learning_rate_mf": {"initial": 0.5, "bounds": [0, 1]},
                     "temperature": {"initial": 3, "bounds": [0.1, 9.0]}}

recov_agent_names = ['valence_mf', 'valence_mb', 'valence_hyb', 'q_mf', "q_mb", "q_hyb"]


for idx, task in tqdm(enumerate(agent_data['task']), total=len(agent_data['task']), desc="Inferring task data."):
    env = get_env(task)

    for recov_agent in recov_agent_names:
        if recov_agent.split("_")[1] in ["mf", "mb", "hyb"]:
            agent_settings = deepcopy(agent_fixed_params["hybrid"])
            agent_settings["state_space"] = env.n_states
            agent_settings["action_space"] = env.n_actions
            agent_settings["graph"] = env.full_graph
            agent_settings["hybrid"] = hybrid_params[recov_agent.split("_")[1]]

        params_free = agent_free_params[recov_agent.split("_")[0]]
        params_name = agent_free_param_names[recov_agent.split("_")[0]]
        rec_agent = agent_class[recov_agent.split("_")[0]]
        

        result = optimize_loglikelihood(agent_data['actions'][idx], 
                               agent_data['rewards'][idx],
                               agent_data['obs0'][idx],
                               agent_data['obs1'][idx],
                               agent_data['terminated'][idx],
                               agent_data['avail-actions'][idx],
                                agent_settings=agent_settings,
                                       parameter_names=params_name,
                                       parameter_settings=params_free,
                                       agent=rec_agent)

        recovery_data['set'].append(idx)
        recovery_data['task'].append(task)
        recovery_data['rcov_agent'].append(recov_agent)
        
        for pc, pn in enumerate(params_name):
            recovery_data[pn].append(result.x[pc])

        for pn in list(set(params_to_track) - set(params_name)):
            recovery_data[pn].append(np.nan)

        recovery_data["lln"].append(result.fun)
        recovery_data["bic"].append(len(params_name) * np.log(len(agent_data['avail-actions'][idx])) + 2 * result.fun)
        recovery_data["orig_params"].append(agent_data['params'][idx])
        recovery_data["orig_agent"].append(agent_data["agent"][idx])    
```

```python
recov_data = pd.DataFrame(recovery_data)
```

```python
sns.catplot(data=recov_data, y="bic", col="orig_agent", x="rcov_agent", row="task", kind="bar")
```
