---
jupyter:
  jupytext:
    formats: ipynb,md
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.3'
      jupytext_version: 1.16.6
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```python
# %pip install -r requirements.txt --upgrade
```

```python
%reload_ext autoreload
%autoreload 2
```

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import rewardgym
from rewardgym import agents, get_env
from rewardgym.psychopy_core import run_task
import seaborn as sns
import warnings
from copy import deepcopy
import os
from typing import Union, Tuple, List
from rewardgym.utils import check_seed, get_stripped_graph

from rewardgym.environments.visualizations import plot_env_graph

from tqdm.auto import tqdm

random_state = np.random.default_rng(2025)

plt.rcParams.update({
    "text.usetex": True,
    "font.family": "Helvetica"
})

```

```python
from model import *
from rl_helper import *
from behavioral_helpers import * 
```

# Setting up your own agent


# Plotting the environment


# Helper functions to run simulation data

We run simple experiments, to see which agents perform best in the task. 


# We here define the parameters for our Simulation

```python
REDO = False
# There are 10 agents: a neutral, a pessimistic and an optimistic agent x a model based, model free and a hybrid agent and a random agent.
alpha_sarsa_pos = [0.4, 0.6, 0.8]
alpha_sarsa_neg = [0.8, 0.6, 0.4] 
# Names:
valence_names = ['pessimistic', 'neutral', 'optimistic']

mb_mf_weighting = [0.0, 1.0] 
model_names = ['model-free', 'model-based']

# Random agent name
ragents_names = ['random']

# Fixed parameters:
discounting = 1.0
temperature = 5
alpha_forward = 0.0 # We assume that the agent knows the task structure.

# For simulation
tasks = ['risk-sensitive', 'two-step']

# Data we collect for our simulation study:
simulation_data_core = ['task', 'agent', 'reward', 'params', "agent_model", "agent_valence"]
simulation_data_behav = ['actions', 'rewards', 'obs0', 'obs1', 'terminated', 'avail-actions']

# Number of agents to simulate per agent class:
n_agents = 5
```

# Simulation

```python
# Create a dictionary containing the necessary info:
random_state = np.random.default_rng(2025)

if not os.path.isfile("rl_simulation.npy") or REDO:
    agent_data = {i : [] for i in simulation_data_core + simulation_data_behav}
    
    for task in tasks:
        for _ in range(n_agents):
            env = get_env(task)
    
            agent_names = []
            agents = []
            params = []
    
            for lrp, lrn, valn in zip(alpha_sarsa_pos, alpha_sarsa_neg, valence_names):
                for we, wen in zip(mb_mf_weighting, model_names):
                    agents.append(ValenceHybridAgent(
                                alpha_mf_neg=lrn, 
                                alpha_mf_pos=lrp, 
                                alpha_mb=alpha_forward, 
                                temperature=temperature, 
                                discount_factor=discounting, 
                                state_space=env.n_states, 
                                hybrid=we,
                                graph=env.full_graph,
                                use_fixed=True,
                                action_space=env.n_actions,
                    seed=random_state))
    
                    agent_names.append(f'{valn}_{wen}')
                    params.append((lrn, lrp, we))
    
            ragents = [RandomAgent(action_space=env.n_actions, state_space=env.n_states, seed=random_state)]
    
            for ag, agn, par in zip(agents + ragents, agent_names + ragents_names, params + [None]):
                    env = get_env(task, seed=random_state)
                    acti, rew, obs0, obs1, term, avail = run_episodes(env, ag, seed_int=random_state.integers(10_000))
                    agent_data['task'].append(task)
                    agent_data['agent'].append(agn)
                    agent_data['agent_model'].append(agn.split('_')[1] if agn != "random" else "random")
                    agent_data['agent_valence'].append(agn.split('_')[0] if agn != "random" else "random")
                    agent_data['reward'].append(np.sum(rew))
                    agent_data['params'].append(par)
        
                    for dat, mes in zip([acti, rew, obs0, obs1, term, avail], simulation_data_behav):
                        agent_data[mes].append(dat)

        np.save('rl_simulation', agent_data)
        agent_data_sim = agent_data
else:
    agent_data_sim = np.load('rl_simulation.npy', allow_pickle=True).item()
```

```python

```

# Agent performance on task

```python
agent_df = pd.DataFrame(agent_data_sim)
fig, axes = plt.subplots(1, 2, figsize=(15, 5))
axes = axes.flatten()

sns.barplot(agent_df.query("task == 'risk-sensitive'"), x='agent_valence', y="reward", ax=axes[0])
sns.barplot(agent_df.query("task == 'two-step'"), x='agent_model', y="reward", ax=axes[1])

titles=['Risk-sensitive', 'Two-step']

for ii in range(2):
    axes[ii].spines['top'].set_visible(False)
    axes[ii].spines['right'].set_visible(False)
    axes[ii].set(ylabel='total reward', xlabel='agent', title=titles[ii])


plt.savefig("rl_simulation_performance.svg", bbox_inches='tight', dpi=600)
```

# Parameter and model recovery

## Optimization is part of rewardCoach


## Optimization loop

```python
import numpy as np
from copy import deepcopy
from tqdm import tqdm

# Parameters to track
params_to_track = ['alpha_mf_pos', 'alpha_mf_neg', 'alpha_mf', 'hybrid']


# Setting up parameter dictionaries
hybrid_params = {"mf": 0.0, "mb": 1.0}
agent_class = {"valence": ValenceHybridAgent, "q": HybridAgent, 'full':ValenceHybridAgent}

agent_fixed_params = {
    "hybrid": {
        "alpha_mb": 0,
        "discount_factor": 1.0,
        "use_fixed": True,
        "hybrid": 1.0,
        "temperature": 5.0,
    }
}

agent_free_param_names = {
    "valence": ["alpha_mf_pos", "alpha_mf_neg"],
    "q": ["alpha_mf"],
    "full": ["alpha_mf_pos", "alpha_mf_neg", "hybrid"]
}

agent_free_params = {
    "valence": {
        "alpha_mf_pos": {"initial": 0.5, "bounds": [0, 1]},
        "alpha_mf_neg": {"initial": 0.5, "bounds": [0, 1]}
    },
    "q": {
        "alpha_mf": {"initial": 0.5, "bounds": [0, 1]},},
    "full":
        {
        "alpha_mf_pos": {"initial": 0.5, "bounds": [0, 1]},
        "alpha_mf_neg": {"initial": 0.5, "bounds": [0, 1]},
        "hybrid": {"initial": 0.5, "bounds": [0, 1]},
        }
    }

recov_agent_names = ['valence_mf', 'valence_mb', 'q_mf', "q_mb", "full"]

def initialize_agent_settings(env, agent_type):
    settings = deepcopy(agent_fixed_params["hybrid"])
    settings["state_space"] = env.n_states
    settings["action_space"] = env.n_actions
    settings["graph"] = env.full_graph
    if agent_type in ['mb', 'mf']:
        settings["hybrid"] = hybrid_params[agent_type]

    return settings

def update_recovery_data(recovery_data, idx, task, recov_agent, result, params_name, agent_data):
    recovery_data['set'].append(idx)
    recovery_data['task'].append(task)
    recovery_data['rcov_agent'].append(recov_agent)

    for pc, pn in enumerate(params_name):
        recovery_data[pn].append(result.x[pc])

    for pn in set(params_to_track) - set(params_name):
        recovery_data[pn].append(np.nan)
    ag_val, ag_model = safe_split(recov_agent)
    recovery_data["lln"].append(result.fun)
    recovery_data["bic"].append(len(params_name) * np.log(len(agent_data['avail-actions'][idx])) + 2 * result.fun)
    recovery_data["orig_params"].append(agent_data['params'][idx])
    recovery_data["orig_agent"].append(agent_data["agent"][idx])
    recovery_data['orig_agent_model'].append(agent_data['agent_model'][idx])
    recovery_data['orig_agent_valence'].append(agent_data['agent_valence'][idx])
    recovery_data['recov_agent_model'].append(ag_model)
    recovery_data['recov_agent_valence'].append(ag_val)


def safe_split(name, split_str="_"):
    if len(name.split("_"))==2:
        return name.split("_")
    elif len(name.split("_"))==1:
        return name, name


if not os.path.isfile("pm_recovery.npy") or REDO:
    # Initialize recovery data dictionary
    recovery_data = {key: [] for key in [
        "task", "set", "rcov_agent", "alpha_mf_pos", "alpha_mf_neg",
        "alpha_mf", "hybrid", "lln", "bic", "orig_params", "orig_agent",
        "orig_agent_model", "orig_agent_valence", "recov_agent_model", "recov_agent_valence"
    ]}
    
    # Main loop
    for idx, task in tqdm(enumerate(agent_data_sim['task']), total=len(agent_data_sim['task']), desc="Inferring task data"):
        env = get_env(task)
    
        for recov_agent in recov_agent_names:
            agent_class_type, agent_type = safe_split(recov_agent)
            if agent_type in ["mf", "mb", "full"]:
                agent_settings = initialize_agent_settings(env, agent_type)
                if agent_type == "full":
                    agent_settings.pop("hybrid")
    
            params_free = agent_free_params[agent_class_type]
            params_name = agent_free_param_names[agent_class_type]
            rec_agent = agent_class[agent_class_type]
    
            result = optimize_loglikelihood(
                agent_data_sim['actions'][idx],
                agent_data_sim['rewards'][idx],
                agent_data_sim['obs0'][idx],
                agent_data_sim['obs1'][idx],
                agent_data_sim['terminated'][idx],
                agent_data_sim['avail-actions'][idx],
                agent_settings=agent_settings,
                parameter_names=params_name,
                parameter_settings=params_free,
                agent=rec_agent
            )
    
            update_recovery_data(recovery_data, idx, task, recov_agent, result, params_name, agent_data_sim)

    np.save("pm_recovery", recovery_data)
else:
    recovery_data = np.load("pm_recovery.npy", allow_pickle=True).item()
```

```python
recov_data = pd.DataFrame(recovery_data)

task_recov_rs = recov_data.query("task=='risk-sensitive' and orig_agent_valence in ['pessimistic', 'neutral', 'optimistic'] and recov_agent_valence == 'full'")
original_learning_rate_pos = [task_recov_rs.iloc[i, :]["orig_params"][1] for i in range(task_recov_rs.shape[0])]
original_learning_rate_neg = [task_recov_rs.iloc[i, :]["orig_params"][0] for i in range(task_recov_rs.shape[0])]


task_recov_ts = recov_data.query("task=='two-step' and orig_agent_valence in ['pessimistic', 'neutral', 'optimistic'] and recov_agent_valence == 'full'")
original_hybrid = [task_recov_ts.iloc[i, :]["orig_params"][2] for i in range(task_recov_ts.shape[0])]


```

```python
def calc_conf_matrix(task, agent_orig, agent_recov, normalize=True):
    
    recov_data_by_set = recov_data.query("task==@task").groupby([agent_orig, agent_recov, 'set'])[['bic', 'lln']].mean().reset_index()
    min_bic_df_idx = recov_data_by_set.groupby("set")["bic"].idxmin()
    min_bic_df = recov_data_by_set.loc[min_bic_df_idx]
    classes = min_bic_df[agent_orig].unique()
    recovs = min_bic_df[agent_recov].unique()
    
    conf_matrix = pd.DataFrame(0, index=recovs, columns=classes)
    
    # Populate the confusion matrix
    for orig, recov in zip(min_bic_df[agent_orig], min_bic_df[agent_recov]):
        conf_matrix.loc[recov, orig] += 1

    if normalize:
        conf_matrix = conf_matrix / conf_matrix.sum(0)

    return conf_matrix
```

```python
fig, axes = plt.subplots(2, 2, figsize=(10, 7.5))

axes = axes.flatten()

sns.regplot(x=original_learning_rate_neg, y=task_recov_rs.alpha_mf_neg, ax=axes[0], label=r'$\alpha^-$')
sns.regplot(x=original_learning_rate_pos, y=task_recov_rs.alpha_mf_pos, ax=axes[0], label=r'$\alpha^+$')

sns.regplot(x=original_hybrid, y=task_recov_ts.hybrid, ax=axes[1])


titles = ['Risk-sensitive:\nlearning rates', 'Two-Step:\nweighting parameter', 'Risk-sensitive\nModel recovery', 'Two-step\nModel recovery']
for ii in range(2):
    axes[ii].spines['top'].set_visible(False)
    axes[ii].spines['right'].set_visible(False)
    axes[ii].set(xlabel='original', ylabel='recovered', title=titles[ii],xlim=[0, 1], ylim=[0, 1])

    if ii == 0:
        axes[ii].legend()


sns.heatmap(calc_conf_matrix("risk-sensitive", "orig_agent_valence", "recov_agent_valence"), annot=True, cmap="viridis", cbar=False, ax=axes[2])
sns.heatmap(calc_conf_matrix("two-step", "orig_agent_model", "recov_agent_model"), annot=True, cmap="viridis", cbar=False, ax=axes[3])

for ii in range(2, 4):
    axes[ii].set(
        title=titles[ii],
        xlabel="Generative model",
        ylabel="Recovered Model",
    )

plt.tight_layout()

plt.savefig("rl_recovery.svg", bbox_inches='tight', dpi=600)
```

# Simulating actual data frames

```python
from behavioral_helpers import *# There are 10 agents: a neutral, a pessimistic and an optimistic agent x a model based, model free and a hybrid agent and a random agent.
# Create a dictionary containing the necessary info:
from rewardgym.psychopy_render.logger import SimulationLogger
random_state = np.random.default_rng(2025)

simulation_data_core = ['task', 'agent', 'reward', 'params', "agent_model", "agent_valence"]
simulation_data_behav = ['dataframes']

if not os.path.isfile("behavioral_simulation.npy") or REDO:
    agent_data = {i : [] for i in simulation_data_core + simulation_data_behav}
    
    for task in tasks:
        for n in range(n_agents):
            env = get_env(task)
    
            agent_names = []
            agents = []
            params = []

            for lrp, lrn, valn in zip(alpha_sarsa_pos, alpha_sarsa_neg, valence_names):
                for we, wen in zip(mb_mf_weighting, model_names):
                    agents.append(ValenceHybridAgent(
                                alpha_mf_neg=lrn, 
                                alpha_mf_pos=lrp, 
                                alpha_mb=alpha_forward, 
                                temperature=temperature, 
                                discount_factor=discounting, 
                                state_space=env.n_states, 
                                hybrid=we,
                                graph=env.full_graph,
                                use_fixed=True,
                                eligiblity_decay=1.0,
                                action_space=env.n_actions,
                    seed=random_state))
    
                    agent_names.append(f'{valn}_{wen}')
                    params.append((lrn, lrp, we))
    
            ragents = [RandomAgent(action_space=env.n_actions, state_space=env.n_states, seed=random_state)]
            
            for ag, agn, par in zip(agents + ragents, agent_names + ragents_names, params + [None]):
                    env = get_env(task, seed=random_state, render_backend='psychopy-simulate')
                    simlog = SimulationLogger(file_name="blank", task=task, participant_id=n)
                    simlog.create()
                    env.setup(logger=simlog, window=None, expose_last_stim=True)
                    df, _, _ = run_task(env, logger=simlog, agent=ag, seed=n)
                    agent_data['task'].append(task)
                    agent_data['agent'].append(agn)
                    agent_data['agent_model'].append(agn.split('_')[1] if agn != "random" else "random")
                    agent_data['agent_valence'].append(agn.split('_')[0] if agn != "random" else "random")
                    agent_data['dataframes'].append(pd.DataFrame(df.close()))
                
    np.save("behavioral_simulation", agent_data)
else:
    agent_data = np.load("behavioral_simulation.npy", allow_pickle=True).item()
```

```python
summary_dfs = {ii : [] for ii in ['risk-sensitive', 'two-step']}

for df, task, ag, agm, agv in zip(agent_data['dataframes'], agent_data['task'], agent_data['agent'], agent_data['agent_model'], agent_data['agent_valence']):

    if task == 'risk-sensitive': 
        rs_men = add_risk_sensitive_meaning(df)
        sum_df = summary_df_risk_sensitive(rs_men, ag)
        
    
    elif task == 'two-step':
        ts_men = add_twostep_meaning(df)
        sum_df = summary_twostep_df(ts_men, ag)

    sum_df['agent_valence'] = agv
    sum_df['agent_model'] = agm
    
    summary_dfs[task].append(sum_df)


rs_df_cc = pd.concat(summary_dfs['risk-sensitive'], ignore_index=False)
ts_df_cc = pd.concat(summary_dfs['two-step'], ignore_index=False)
```

```python
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

xorder = ['expected', 'unexpected']
horder = ['reward', 'no-reward']
ts_models = ['Two-step:\nmodel-free', 'Two-step:\nmodel-based']

rs_df_cc = pd.concat(summary_dfs['risk-sensitive'], ignore_index=False)
ts_df_cc = pd.concat(summary_dfs['two-step'], ignore_index=False)

plot_df = ts_df_cc.query("metric == 'proportion'")
plot_df.loc[:, ['reward', 'transition']] = plot_df.trial_type.str.split('_', expand=True).values


sns.barplot(plot_df.query("agent_model=='model-free'"), x='transition', hue='reward', order=xorder, hue_order=horder, y='value', ax=axes[1])
sns.barplot(plot_df.query("agent_model=='model-based'"), x='transition', hue='reward', order=xorder, hue_order=horder, y='value', ax=axes[2])

sns.barplot(rs_df_cc.query("metric=='proportion' and trial_classification=='risky'"), x='agent_valence', y='value', ax=axes[0])

for ii in range(3):
    if ii in [1, 2]:
        axes[ii].set(ylabel='proportion stay', title=ts_models[ii - 1], xlabel='previous transition')
    axes[ii].spines['top'].set_visible(False)
    axes[ii].spines['right'].set_visible(False)
    axes[ii].tick_params(axis='x', labelrotation=45)
    if ii == 2:
        axes[ii].legend().set_visible(False)
    elif ii == 1:
        axes[ii].legend(title='previous reward')


    if ii==0:
        axes[ii].set(ylabel='proportion safe responses', title='Risk-sensitive:\nRisk-taking', xlabel='agent')

plt.savefig("behavioral_simulation.svg", bbox_inches='tight', dpi=600)
```

```python

```

```python

```

```python

```

```python

```
